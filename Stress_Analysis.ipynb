{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stress_Analysis.ipynb",
      "provenance": [],
      "mount_file_id": "1niXV4FAJfHVdCKppv5Vw3NGZqYqiUEOV",
      "authorship_tag": "ABX9TyMpT55TgSx0i/Fwq9knFk1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tramyynt/Stress-Analysis/blob/main/Stress_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzs04t9-3vC9"
      },
      "source": [
        "Using Text Mining, NLP and Classification algorithms to classify a comment on Reddit is stressed or non-stressed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB_QdiRSJXlZ"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvvimFK42Wu3"
      },
      "source": [
        "#import lib\n",
        "! pip install -q kaggle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy import spatial\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "b88bb9qSEwwO",
        "outputId": "333a786b-9b52-4dd1-c8a3-f56d60ed208e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-304e8af8-d2c3-44fe-9292-bae26a2b53ce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-304e8af8-d2c3-44fe-9292-bae26a2b53ce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tramyynt26\",\"key\":\"d3e9f3fd41fa1c8ba08273c29eca6c96\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEYrFiNCFPaY"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK81EtS4FYgf"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axNDJbIEFg3q",
        "outputId": "cbf866fc-293c-4d4e-ebc3-8de5eb4c8499"
      },
      "source": [
        "! kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              233KB  2021-08-19 07:30:16          10892  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           6667  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19           2517  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   2GB  2021-07-03 18:37:20           2677  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52           1645  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00           1150  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04            787  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23            965  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37            845  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39            290  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            719  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            295  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54            134  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40            175  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26            124  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51            122  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         146831  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         142139  \n",
            "residentmario/ramen-ratings                                 Ramen Ratings                                      40KB  2018-01-11 16:04:39          25876  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          20873  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBj-7hlNJLy0",
        "outputId": "4f3e1c30-c767-4d0c-fb57-cd4f4c7eb10f"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 2.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=7feb0b6c4834fcdd237755c49a02b00c838175d07a4a4c0e34d03c883ec21bc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yUiOHwSJcNs",
        "outputId": "1375fbcd-7f6e-464d-ff35-458c0e84c9c8"
      },
      "source": [
        "!kaggle datasets download ruchi798/stress-analysis-in-social-media -p /content/gdrive/kaggle"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stress-analysis-in-social-media.zip to /content/gdrive/kaggle\n",
            "\r  0% 0.00/1.31M [00:00<?, ?B/s]\n",
            "\r100% 1.31M/1.31M [00:00<00:00, 123MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs2YRvySTpPo",
        "outputId": "55397292-48e2-4443-ec48-905a814c7563"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/drive/')\n",
        "\n",
        "!unzip /content/gdrive/kaggle/stress-analysis-in-social-media.zip -d '/content/gdrive/kaggle'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/kaggle/stress-analysis-in-social-media.zip\n",
            "  inflating: /content/gdrive/kaggle/dreaddit-test.csv  \n",
            "  inflating: /content/gdrive/kaggle/dreaddit-train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD2BhO8MWWsc"
      },
      "source": [
        "train = pd.read_csv('/content/gdrive/kaggle/dreaddit-train.csv')\n",
        "test = pd.read_csv('/content/gdrive/kaggle/dreaddit-test.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8GalAnUap2N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "6538acf4-de3e-425b-c242-802694aa86d8"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subreddit</th>\n",
              "      <th>post_id</th>\n",
              "      <th>sentence_range</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>confidence</th>\n",
              "      <th>social_timestamp</th>\n",
              "      <th>social_karma</th>\n",
              "      <th>syntax_ari</th>\n",
              "      <th>lex_liwc_WC</th>\n",
              "      <th>lex_liwc_Analytic</th>\n",
              "      <th>lex_liwc_Clout</th>\n",
              "      <th>lex_liwc_Authentic</th>\n",
              "      <th>lex_liwc_Tone</th>\n",
              "      <th>lex_liwc_WPS</th>\n",
              "      <th>lex_liwc_Sixltr</th>\n",
              "      <th>lex_liwc_Dic</th>\n",
              "      <th>lex_liwc_function</th>\n",
              "      <th>lex_liwc_pronoun</th>\n",
              "      <th>lex_liwc_ppron</th>\n",
              "      <th>lex_liwc_i</th>\n",
              "      <th>lex_liwc_we</th>\n",
              "      <th>lex_liwc_you</th>\n",
              "      <th>lex_liwc_shehe</th>\n",
              "      <th>lex_liwc_they</th>\n",
              "      <th>lex_liwc_ipron</th>\n",
              "      <th>lex_liwc_article</th>\n",
              "      <th>lex_liwc_prep</th>\n",
              "      <th>lex_liwc_auxverb</th>\n",
              "      <th>lex_liwc_adverb</th>\n",
              "      <th>lex_liwc_conj</th>\n",
              "      <th>lex_liwc_negate</th>\n",
              "      <th>lex_liwc_verb</th>\n",
              "      <th>lex_liwc_adj</th>\n",
              "      <th>lex_liwc_compare</th>\n",
              "      <th>lex_liwc_interrog</th>\n",
              "      <th>lex_liwc_number</th>\n",
              "      <th>lex_liwc_quant</th>\n",
              "      <th>lex_liwc_affect</th>\n",
              "      <th>...</th>\n",
              "      <th>lex_liwc_motion</th>\n",
              "      <th>lex_liwc_space</th>\n",
              "      <th>lex_liwc_time</th>\n",
              "      <th>lex_liwc_work</th>\n",
              "      <th>lex_liwc_leisure</th>\n",
              "      <th>lex_liwc_home</th>\n",
              "      <th>lex_liwc_money</th>\n",
              "      <th>lex_liwc_relig</th>\n",
              "      <th>lex_liwc_death</th>\n",
              "      <th>lex_liwc_informal</th>\n",
              "      <th>lex_liwc_swear</th>\n",
              "      <th>lex_liwc_netspeak</th>\n",
              "      <th>lex_liwc_assent</th>\n",
              "      <th>lex_liwc_nonflu</th>\n",
              "      <th>lex_liwc_filler</th>\n",
              "      <th>lex_liwc_AllPunc</th>\n",
              "      <th>lex_liwc_Period</th>\n",
              "      <th>lex_liwc_Comma</th>\n",
              "      <th>lex_liwc_Colon</th>\n",
              "      <th>lex_liwc_SemiC</th>\n",
              "      <th>lex_liwc_QMark</th>\n",
              "      <th>lex_liwc_Exclam</th>\n",
              "      <th>lex_liwc_Dash</th>\n",
              "      <th>lex_liwc_Quote</th>\n",
              "      <th>lex_liwc_Apostro</th>\n",
              "      <th>lex_liwc_Parenth</th>\n",
              "      <th>lex_liwc_OtherP</th>\n",
              "      <th>lex_dal_max_pleasantness</th>\n",
              "      <th>lex_dal_max_activation</th>\n",
              "      <th>lex_dal_max_imagery</th>\n",
              "      <th>lex_dal_min_pleasantness</th>\n",
              "      <th>lex_dal_min_activation</th>\n",
              "      <th>lex_dal_min_imagery</th>\n",
              "      <th>lex_dal_avg_activation</th>\n",
              "      <th>lex_dal_avg_imagery</th>\n",
              "      <th>lex_dal_avg_pleasantness</th>\n",
              "      <th>social_upvote_ratio</th>\n",
              "      <th>social_num_comments</th>\n",
              "      <th>syntax_fk_grade</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>8601tu</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>He said he had not felt that way before, sugge...</td>\n",
              "      <td>33181</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1521614353</td>\n",
              "      <td>5</td>\n",
              "      <td>1.806818</td>\n",
              "      <td>116</td>\n",
              "      <td>72.64</td>\n",
              "      <td>15.04</td>\n",
              "      <td>89.26</td>\n",
              "      <td>1.00</td>\n",
              "      <td>29.00</td>\n",
              "      <td>12.93</td>\n",
              "      <td>87.07</td>\n",
              "      <td>56.03</td>\n",
              "      <td>16.38</td>\n",
              "      <td>12.07</td>\n",
              "      <td>9.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.31</td>\n",
              "      <td>3.45</td>\n",
              "      <td>19.83</td>\n",
              "      <td>7.76</td>\n",
              "      <td>5.17</td>\n",
              "      <td>4.31</td>\n",
              "      <td>1.72</td>\n",
              "      <td>16.38</td>\n",
              "      <td>6.03</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.72</td>\n",
              "      <td>8.62</td>\n",
              "      <td>...</td>\n",
              "      <td>0.86</td>\n",
              "      <td>10.34</td>\n",
              "      <td>6.03</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.55</td>\n",
              "      <td>9.48</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.17</td>\n",
              "      <td>1.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.8571</td>\n",
              "      <td>2.6250</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.77000</td>\n",
              "      <td>1.52211</td>\n",
              "      <td>1.89556</td>\n",
              "      <td>0.86</td>\n",
              "      <td>1</td>\n",
              "      <td>3.253573</td>\n",
              "      <td>-0.002742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assistance</td>\n",
              "      <td>8lbrx9</td>\n",
              "      <td>(0, 5)</td>\n",
              "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
              "      <td>2606</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1527009817</td>\n",
              "      <td>4</td>\n",
              "      <td>9.429737</td>\n",
              "      <td>109</td>\n",
              "      <td>79.08</td>\n",
              "      <td>76.85</td>\n",
              "      <td>56.75</td>\n",
              "      <td>98.18</td>\n",
              "      <td>27.25</td>\n",
              "      <td>21.10</td>\n",
              "      <td>87.16</td>\n",
              "      <td>48.62</td>\n",
              "      <td>11.93</td>\n",
              "      <td>7.34</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2.75</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.59</td>\n",
              "      <td>8.26</td>\n",
              "      <td>13.76</td>\n",
              "      <td>6.42</td>\n",
              "      <td>3.67</td>\n",
              "      <td>8.26</td>\n",
              "      <td>0.92</td>\n",
              "      <td>15.60</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.92</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.92</td>\n",
              "      <td>5.50</td>\n",
              "      <td>...</td>\n",
              "      <td>2.75</td>\n",
              "      <td>10.09</td>\n",
              "      <td>1.83</td>\n",
              "      <td>11.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.83</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.68</td>\n",
              "      <td>4.59</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.92</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>2.8889</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.69586</td>\n",
              "      <td>1.62045</td>\n",
              "      <td>1.88919</td>\n",
              "      <td>0.65</td>\n",
              "      <td>2</td>\n",
              "      <td>8.828316</td>\n",
              "      <td>0.292857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ptsd</td>\n",
              "      <td>9ch1zh</td>\n",
              "      <td>(15, 20)</td>\n",
              "      <td>My mom then hit me with the newspaper and it s...</td>\n",
              "      <td>38816</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1535935605</td>\n",
              "      <td>2</td>\n",
              "      <td>7.769821</td>\n",
              "      <td>167</td>\n",
              "      <td>33.80</td>\n",
              "      <td>76.38</td>\n",
              "      <td>86.24</td>\n",
              "      <td>25.77</td>\n",
              "      <td>33.40</td>\n",
              "      <td>17.37</td>\n",
              "      <td>91.02</td>\n",
              "      <td>61.68</td>\n",
              "      <td>25.15</td>\n",
              "      <td>16.17</td>\n",
              "      <td>8.98</td>\n",
              "      <td>1.80</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.6</td>\n",
              "      <td>8.98</td>\n",
              "      <td>5.39</td>\n",
              "      <td>12.57</td>\n",
              "      <td>10.18</td>\n",
              "      <td>1.80</td>\n",
              "      <td>5.99</td>\n",
              "      <td>1.20</td>\n",
              "      <td>20.96</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.20</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.40</td>\n",
              "      <td>...</td>\n",
              "      <td>4.79</td>\n",
              "      <td>5.99</td>\n",
              "      <td>5.39</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.78</td>\n",
              "      <td>2.40</td>\n",
              "      <td>3.59</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.40</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.7143</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1429</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.83088</td>\n",
              "      <td>1.58108</td>\n",
              "      <td>1.85828</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0</td>\n",
              "      <td>7.841667</td>\n",
              "      <td>0.011894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relationships</td>\n",
              "      <td>7rorpp</td>\n",
              "      <td>[5, 10]</td>\n",
              "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
              "      <td>239</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1516429555</td>\n",
              "      <td>0</td>\n",
              "      <td>2.667798</td>\n",
              "      <td>273</td>\n",
              "      <td>2.98</td>\n",
              "      <td>15.25</td>\n",
              "      <td>95.42</td>\n",
              "      <td>79.26</td>\n",
              "      <td>54.60</td>\n",
              "      <td>8.06</td>\n",
              "      <td>98.90</td>\n",
              "      <td>65.57</td>\n",
              "      <td>30.40</td>\n",
              "      <td>23.44</td>\n",
              "      <td>16.12</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.37</td>\n",
              "      <td>6.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>3.30</td>\n",
              "      <td>9.16</td>\n",
              "      <td>8.79</td>\n",
              "      <td>6.59</td>\n",
              "      <td>9.89</td>\n",
              "      <td>3.66</td>\n",
              "      <td>20.88</td>\n",
              "      <td>3.66</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.10</td>\n",
              "      <td>8.79</td>\n",
              "      <td>...</td>\n",
              "      <td>1.83</td>\n",
              "      <td>3.30</td>\n",
              "      <td>6.23</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.09</td>\n",
              "      <td>2.56</td>\n",
              "      <td>7.33</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>2.6364</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.75356</td>\n",
              "      <td>1.52114</td>\n",
              "      <td>1.98848</td>\n",
              "      <td>0.50</td>\n",
              "      <td>5</td>\n",
              "      <td>4.104027</td>\n",
              "      <td>0.141671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>survivorsofabuse</td>\n",
              "      <td>9p2gbc</td>\n",
              "      <td>[0, 5]</td>\n",
              "      <td>October is Domestic Violence Awareness Month a...</td>\n",
              "      <td>1421</td>\n",
              "      <td>1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1539809005</td>\n",
              "      <td>24</td>\n",
              "      <td>7.554238</td>\n",
              "      <td>89</td>\n",
              "      <td>32.22</td>\n",
              "      <td>28.71</td>\n",
              "      <td>84.01</td>\n",
              "      <td>1.00</td>\n",
              "      <td>17.80</td>\n",
              "      <td>31.46</td>\n",
              "      <td>88.76</td>\n",
              "      <td>52.81</td>\n",
              "      <td>15.73</td>\n",
              "      <td>11.24</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.49</td>\n",
              "      <td>4.49</td>\n",
              "      <td>8.99</td>\n",
              "      <td>13.48</td>\n",
              "      <td>4.49</td>\n",
              "      <td>4.49</td>\n",
              "      <td>2.25</td>\n",
              "      <td>13.48</td>\n",
              "      <td>4.49</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1.12</td>\n",
              "      <td>1.12</td>\n",
              "      <td>1.12</td>\n",
              "      <td>7.87</td>\n",
              "      <td>...</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2.25</td>\n",
              "      <td>10.11</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.85</td>\n",
              "      <td>5.62</td>\n",
              "      <td>6.74</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.12</td>\n",
              "      <td>2.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1.1250</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.77644</td>\n",
              "      <td>1.64872</td>\n",
              "      <td>1.81456</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1</td>\n",
              "      <td>7.910952</td>\n",
              "      <td>-0.204167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 116 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          subreddit post_id  ... syntax_fk_grade sentiment\n",
              "0              ptsd  8601tu  ...        3.253573 -0.002742\n",
              "1        assistance  8lbrx9  ...        8.828316  0.292857\n",
              "2              ptsd  9ch1zh  ...        7.841667  0.011894\n",
              "3     relationships  7rorpp  ...        4.104027  0.141671\n",
              "4  survivorsofabuse  9p2gbc  ...        7.910952 -0.204167\n",
              "\n",
              "[5 rows x 116 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fy1zRfPtG7u"
      },
      "source": [
        "subred = df['subreddit']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOyi23a_psp6",
        "outputId": "13df455b-4a78-48a4-b1d2-7fb44f93707e"
      },
      "source": [
        "len(train['text'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxmZvaB7FWaU",
        "outputId": "d546f51d-a966-4bef-82d3-6b67830c6a07"
      },
      "source": [
        "subred.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ptsd', 'assistance', 'relationships', 'survivorsofabuse',\n",
              "       'domesticviolence', 'anxiety', 'homeless', 'stress',\n",
              "       'almosthomeless', 'food_pantry'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpyFDLeTJRtx"
      },
      "source": [
        "## **Data Preprocessing** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-DPLrChVR7f"
      },
      "source": [
        "Data preprocessing before going through vectorizing words.\n",
        "\n",
        "1.   Tokenization, Removal of punctuations.\n",
        "2.   Filtering stopwords\n",
        "3.   Normalization ( Stemming and Lemmatization )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12nnUlDkXeeO"
      },
      "source": [
        "### **Tokenization, Removal of punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2WVG7ibhQkV",
        "outputId": "db4f367a-1c4b-400f-ed43-28e622755c98"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G8-rxpchbcW"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}') #remove number and words has only one character\n",
        "# sr = []\n",
        "# for x in range(0,len(train['text'])):\n",
        "#   dt = tokenizer.tokenize(train['text'][x])\n",
        "#   sr.append(dt)\n",
        "train['processed_text'] = train['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "test['processed_text'] = test['text'].apply(lambda x: tokenizer.tokenize(x.lower()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lNmbTwCsd4O"
      },
      "source": [
        "### **Filtering stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1yKiW3EvVyz",
        "outputId": "e83db426-0c9c-479a-fc87-68aa52442e50"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9Cxdlucvd6D"
      },
      "source": [
        "def filtering_stopwords(data):\n",
        "  words = [w for w in data if not w in stopwords.words('english')]\n",
        "  return words"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ecZGb54woXz"
      },
      "source": [
        "train['processed_text'] = train['processed_text'].apply(lambda x: filtering_stopwords(x))\n",
        "test['processed_text'] = test['processed_text'].apply(lambda x: filtering_stopwords(x))\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I88MdNH10-EJ"
      },
      "source": [
        "### **Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khC-ilPw1B5T"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "def word_stemmer(data):\n",
        "    stem_text = [stemmer.stem(i) for i in data]\n",
        "    return stem_text\n",
        "\n",
        "train['processed_text'] = train['processed_text'].apply(lambda x: word_stemmer(x))\n",
        "test['processed_text'] = test['processed_text'].apply(lambda x: word_stemmer(x))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7iVUhZ0QU-s",
        "outputId": "7d9bf43a-8a20-48f5-a0c8-4fde54ee597e"
      },
      "source": [
        "train['processed_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [said, felt, way, sugget, go, rest, trigger, a...\n",
              "1       [hey, assist, sure, right, place, post, goe, c...\n",
              "2       [mom, hit, newspap, shock, would, know, like, ...\n",
              "3       [met, new, boyfriend, amaz, kind, sweet, good,...\n",
              "4       [octob, domest, violenc, awar, month, domest, ...\n",
              "                              ...                        \n",
              "2833    [week, ago, preciou, ignor, jan, happi, year, ...\n",
              "2834    [abil, cope, anymor, tri, lot, thing, trigger,...\n",
              "2835    [case, first, time, read, post, look, peopl, w...\n",
              "2836    [find, normal, good, relationship, main, probl...\n",
              "2837    [talk, mom, morn, said, sister, trauma, wors, ...\n",
              "Name: processed_text, Length: 2838, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JcrEOBmReY1",
        "outputId": "79272503-788f-4a56-af1e-d64b6291c147"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMtXtWNVQcan"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def word_lemmatizer(data):\n",
        "  lemma_text = [lemmatizer.lemmatize(i) for i in data]\n",
        "  return lemma_text\n",
        "train['processed_text'] = train['processed_text'].apply(lambda x: word_lemmatizer(x))\n",
        "test['processed_text'] = test['processed_text'].apply(lambda x: word_lemmatizer(x))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9U284DxRiPj",
        "outputId": "fb09248b-9abd-4aeb-de05-37a9e4954e93"
      },
      "source": [
        "train['processed_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [said, felt, way, sugget, go, rest, trigger, a...\n",
              "1       [hey, assist, sure, right, place, post, goe, c...\n",
              "2       [mom, hit, newspap, shock, would, know, like, ...\n",
              "3       [met, new, boyfriend, amaz, kind, sweet, good,...\n",
              "4       [octob, domest, violenc, awar, month, domest, ...\n",
              "                              ...                        \n",
              "2833    [week, ago, preciou, ignor, jan, happi, year, ...\n",
              "2834    [abil, cope, anymor, tri, lot, thing, trigger,...\n",
              "2835    [case, first, time, read, post, look, peopl, w...\n",
              "2836    [find, normal, good, relationship, main, probl...\n",
              "2837    [talk, mom, morn, said, sister, trauma, wors, ...\n",
              "Name: processed_text, Length: 2838, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMywoECrWWej"
      },
      "source": [
        "## **Word Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L53b-TtSWz3q"
      },
      "source": [
        "Using Global Vectors for Word Representation ( GLOVE) is an “unsupervised learning algorithm for obtaining vector representations for words \"\n",
        "Reference:\n",
        "https://towardsdatascience.com/understanding-word-embeddings-with-tf-idf-and-glove-8acb63892032\n",
        "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html\n",
        "https://gist.github.com/TomLin/30244bcccb7e4f94d191a878a697f698\n",
        "https://docs.google.com/presentation/d/1Gg0l1GySbVOJ5Uys8gqfGnuHd-zIv6MZR5-ZoqxJxL4/edit#slide=id.g6e630d8087_0_181\n",
        "https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWXBXb0dfnf"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiF4phY3LBTd",
        "outputId": "5a4218f6-605b-4571-f6cd-fcb568330d6d"
      },
      "source": [
        "word2vec_output_file = 'glove.6B.100d.txt'+'.word2vec'\n",
        "glove2word2vec('/content/glove.6B.100d.txt', word2vec_output_file)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH_F69g6LBeV"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbKy0Q87OHEd",
        "outputId": "92d0c1a3-c13d-4761-fb2d-d78462ad0a43"
      },
      "source": [
        "keys = ['abuse', 'anxiety', 'financial', 'ptsd', 'social', 'stress', 'love', 'friendship']\n",
        "for i in keys:\n",
        "  print('Vector of:', i ,model.get_vector(i))\n",
        "  result = model.most_similar(positive=i, topn=10)\n",
        "  print('\\n10 most similar words to: ',i,result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector of: abuse [ 0.6747     0.31766    0.19427   -0.23009   -0.7347     1.4631\n",
            " -0.6046    -0.098757   0.054897   0.23114   -0.074416   0.088503\n",
            " -0.26545    1.1766     0.32394    0.31836    0.073733  -0.019944\n",
            " -1.5462     0.44651   -0.26256   -0.26111   -0.32883    0.60584\n",
            " -0.66182    0.028531   0.61704   -0.34273   -0.20538    0.77077\n",
            "  1.0128    -0.14495    0.0048883  0.63247   -0.26677   -0.19595\n",
            " -0.44454   -0.3079    -0.21728    0.38524   -0.83284    0.18319\n",
            " -0.083784  -0.66435   -0.13656    0.26515    0.34094   -0.27263\n",
            "  0.0086811 -0.76744    0.83997   -0.44614   -0.71315    1.2288\n",
            "  0.16899   -1.0177    -0.16239   -0.90029    1.3657     0.17637\n",
            "  0.051633  -0.068679   0.16901   -0.95653    0.55789   -0.12813\n",
            "  0.95465   -1.1387     0.61918    0.20519    0.18888    0.029914\n",
            " -0.68286    0.28326    0.36491    0.071344  -0.23957    0.32145\n",
            " -1.1288     0.39188    0.83201    0.45375    0.59285    0.50722\n",
            " -2.0063    -0.72023   -0.12334    0.24201   -0.24597   -0.65065\n",
            " -0.39714   -0.44392   -0.27164    0.042077   1.3559    -0.019485\n",
            " -0.35694   -0.11555    0.53027   -0.3727   ]\n",
            "\n",
            "10 most similar words to:  abuse [('sexual', 0.7727023363113403), ('sex', 0.7388114929199219), ('abuses', 0.7365543842315674), ('allegations', 0.7294367551803589), ('rape', 0.7241652607917786), ('harassment', 0.7186876535415649), ('misconduct', 0.7040334343910217), ('torture', 0.6989931464195251), ('corruption', 0.6851984262466431), ('child', 0.6768218874931335)]\n",
            "Vector of: anxiety [ 0.0066306  0.1828     0.67915    0.10635   -0.98088    0.51839\n",
            " -0.80593   -0.88419    0.14702    0.014602  -0.86182    0.04276\n",
            " -0.51645   -0.37693    0.82099    0.011972  -0.76182    0.042818\n",
            " -0.24961   -0.579     -0.11956   -0.1257    -0.67124    0.02598\n",
            " -0.54218   -0.033592   0.68791   -0.28887    0.086576  -0.12568\n",
            " -0.094109   0.10699   -0.41816    0.21058   -0.67646    0.23273\n",
            "  0.93634    0.36526    0.21686    0.14149   -0.32114   -0.20877\n",
            " -0.25064   -0.69067    0.3278    -0.1785     0.60967    0.33788\n",
            "  0.089491  -0.82491   -0.13942   -0.38754   -0.31975    0.66951\n",
            "  0.55314   -1.4863     0.71065   -0.17214   -0.0026193  0.21807\n",
            "  0.73602    1.1448     0.66654   -0.3916    -0.10016    0.7296\n",
            "  0.69991   -1.138      0.80996   -0.29561   -0.29745   -0.20458\n",
            " -0.71795    0.23659    0.096237   1.1553     0.12702   -0.22309\n",
            " -0.96484    0.013153   0.16529   -0.044983  -0.42449    0.24305\n",
            " -1.3525    -0.15527   -0.23521    0.028487  -0.87109   -0.50601\n",
            " -0.74841    0.3311    -0.075076   0.23237    0.34843    0.778\n",
            "  1.1292    -0.66228    0.20343    0.18071  ]\n",
            "\n",
            "10 most similar words to:  anxiety [('anger', 0.7617689371109009), ('nervousness', 0.7336102724075317), ('frustration', 0.729232907295227), ('discomfort', 0.7233244776725769), ('paranoia', 0.7181074619293213), ('confusion', 0.7138597965240479), ('anxieties', 0.7117573022842407), ('stress', 0.7095138430595398), ('despair', 0.7053972482681274), ('feelings', 0.698227047920227)]\n",
            "Vector of: financial [ 0.33134  -0.04661   0.10657  -0.26801  -0.27547  -0.73264  -0.9775\n",
            " -0.64882   0.062462 -0.20076   0.65747   0.24898   0.10554   0.11204\n",
            " -0.53015  -0.18722   0.27637  -0.33475  -0.23617  -0.20215  -0.41184\n",
            "  0.028952 -0.298     0.16072  -0.66309  -0.34644   0.10096  -0.2311\n",
            " -0.97776   0.28702   0.54741   0.28516  -0.92817  -0.32457  -0.9536\n",
            " -0.31486   0.39549   0.14776  -0.456     0.15904  -0.46332  -0.91914\n",
            " -0.1546    0.13042  -0.1438    0.39351   0.32141   0.085901 -0.2182\n",
            " -0.87695  -0.5807    0.071227  0.19958   0.54646   0.35967  -2.7143\n",
            "  0.44322  -0.56998   1.8921    0.81757  -0.26195  -0.19525  -1.3027\n",
            " -0.099452  0.3325   -0.79673  -0.35013   0.23426   1.3519   -0.16837\n",
            " -0.72781   0.036372 -0.98335  -0.42376   0.1248   -0.39884  -0.38062\n",
            "  0.34085  -1.0434    0.040638  1.5705    0.24799   0.12334   0.043277\n",
            " -1.3124   -0.37536  -0.49102  -0.19736  -0.20121  -0.88124   0.23803\n",
            "  0.47244   0.27429  -0.40834   0.26192   0.99651   0.98947  -0.6365\n",
            "  0.67295   0.62477 ]\n",
            "\n",
            "10 most similar words to:  financial [('banking', 0.7891805171966553), ('corporate', 0.7738927602767944), ('economic', 0.7714813947677612), ('credit', 0.7692809104919434), ('investment', 0.7671132683753967), ('business', 0.7482264041900635), ('global', 0.7365726232528687), ('management', 0.734153151512146), ('fund', 0.7081953883171082), ('banks', 0.7058879137039185)]\n",
            "Vector of: ptsd [-0.323     0.93968  -0.14216   0.07845  -1.5524   -0.31777  -0.13789\n",
            " -0.51181   0.511    -0.10846  -0.30864  -0.41167  -0.097314  0.80191\n",
            "  1.5308    0.041924  0.23779  -0.97669  -0.099634 -0.17963  -0.037075\n",
            " -0.046191 -1.1301    0.8084   -0.61865  -0.45362   0.6355   -0.35339\n",
            "  0.31447  -0.24616   0.60796   0.89891  -0.53141   0.31569   0.25266\n",
            " -0.14764   0.11194   0.59363  -0.64152   0.046084  0.039019  1.1668\n",
            " -0.083849 -0.21478  -0.12963   0.16848   0.74785  -0.01945  -0.47476\n",
            " -0.69543   0.46255  -0.65407  -0.42366  -0.027828  0.44531   0.483\n",
            "  1.235    -0.38235  -0.26627   0.85118   0.2935    1.8751    1.1202\n",
            " -0.32175  -0.010322  0.80081   0.10104  -0.98285   0.015198  0.73697\n",
            "  0.13984   0.60534  -0.76027   0.16573   0.62429   0.14766  -0.51643\n",
            "  0.76707  -0.027361 -0.29336  -0.062604  0.061716  0.075125 -0.096013\n",
            " -0.54888   0.30848   0.56884   0.71552  -0.64725  -0.11532  -0.32666\n",
            "  0.13587   0.6246   -0.18767   0.85372   0.20314   1.0594   -0.043378\n",
            "  0.063829 -0.42204 ]\n",
            "\n",
            "10 most similar words to:  ptsd [('traumatic', 0.7087750434875488), ('post-traumatic', 0.6651941537857056), ('disorder', 0.6327652335166931), ('posttraumatic', 0.6266862750053406), ('fibromyalgia', 0.6164780855178833), ('psychosis', 0.6153751611709595), ('trauma', 0.6145431995391846), ('schizophrenia', 0.6127399206161499), ('adhd', 0.6126563549041748), ('disorders', 0.5918419361114502)]\n",
            "Vector of: social [-0.24861   0.65492  -0.34935   0.22602   0.32131   0.33169  -0.69937\n",
            " -0.79396   0.36112   0.45019  -0.5848   -0.26079  -0.20359   0.11051\n",
            "  0.56347  -0.28351   0.69255   0.061113 -0.40681   0.17453  -0.26842\n",
            " -0.044954  0.22822   0.14171  -0.42381  -1.0615   -0.099736 -0.61057\n",
            " -0.16646  -0.10446  -0.21737   0.94042  -0.30025  -0.067251 -0.40675\n",
            " -0.1696    0.19566   0.61129  -0.75535   0.34861  -0.81396  -0.44849\n",
            " -0.13668   0.11478  -0.6047   -0.33903   0.039049  0.45745  -0.60466\n",
            "  0.22668   0.26345  -0.2814   -0.54628   0.87143   0.46428  -2.198\n",
            "  1.1758   -0.039133  1.8234    0.88758   0.12617  -0.23075  -0.91688\n",
            " -0.59195   1.3559    0.40534  -0.047916 -0.63889   1.8571   -0.27157\n",
            "  0.061506 -0.016188 -0.1973   -0.45165  -0.38345  -0.067396 -0.30852\n",
            " -0.15313  -0.57111  -0.048921 -0.20939   0.31559   0.58533  -0.10424\n",
            " -1.8253   -0.030741 -0.69598   0.2492   -0.53853  -1.1237    0.6469\n",
            " -0.55666   0.27738   0.57313  -0.038488  0.056588  0.24121  -0.96586\n",
            "  0.79163   0.12448 ]\n",
            "\n",
            "10 most similar words to:  social [('education', 0.7497825622558594), ('political', 0.7463673949241638), ('cultural', 0.7148115038871765), ('welfare', 0.7065821886062622), ('educational', 0.6951513886451721), ('reform', 0.6909915208816528), ('health', 0.6869200468063354), ('public', 0.6821627616882324), ('environment', 0.6808215379714966), ('environmental', 0.6754920482635498)]\n",
            "Vector of: stress [-0.79252    0.1112    -0.15714    0.33814   -1.5105    -0.2456\n",
            " -1.0716    -0.35966   -0.62459    0.21511   -0.35896   -0.38863\n",
            "  0.15035    0.47511    0.34397    0.092767  -0.76359   -0.32845\n",
            " -0.18058   -0.69679    0.025618  -0.22498   -1.0008     0.44016\n",
            " -0.42199    0.2957    -0.15925   -0.46405    0.37978   -0.035413\n",
            " -0.2422     0.15262   -0.27078    0.062017  -0.17211   -0.24836\n",
            "  0.4039     0.4822    -0.44304    0.40773   -0.44366    0.088745\n",
            " -0.022201  -0.49345   -0.31535    0.26685    0.041346   0.10019\n",
            " -0.30001   -1.0617     0.19757   -0.77107   -0.23166    0.75719\n",
            "  0.7147    -1.4065     0.53922   -0.85026    0.8112     0.93463\n",
            "  0.38038    0.85097   -0.096784  -0.44867    0.74315    0.81144\n",
            "  0.13238   -0.76748    0.42411   -0.3287    -0.43828    0.1506\n",
            " -0.077817   0.30273    0.63116    0.67563    0.14995   -0.34982\n",
            " -0.17312    0.11598    0.28339   -0.35725   -0.88685   -0.0050747\n",
            " -1.6313     0.40505    0.23601    0.12228   -1.0897    -0.28835\n",
            " -0.65577   -0.2405     0.12842    0.33902    0.81716    0.31335\n",
            "  0.62835   -1.1465     0.6587     0.12363  ]\n",
            "\n",
            "10 most similar words to:  stress [('pain', 0.7258723378181458), ('anxiety', 0.7095138430595398), ('fatigue', 0.7049359679222107), ('mental', 0.7017255425453186), ('psychological', 0.6904555559158325), ('physical', 0.6795199513435364), ('severe', 0.6727945804595947), ('problems', 0.6613970398902893), ('risk', 0.6541153788566589), ('muscle', 0.6459755897521973)]\n",
            "Vector of: love [ 2.5975e-01  5.5833e-01  5.7986e-01 -2.1361e-01  1.3084e-01  9.4385e-01\n",
            " -4.2817e-01 -3.7420e-01 -9.4499e-02 -4.3344e-01 -2.0937e-01  3.4702e-01\n",
            "  8.2516e-02  7.9735e-01  1.6606e-01 -2.6878e-01  5.8830e-01  6.7397e-01\n",
            " -4.9965e-01  1.4764e+00  5.5261e-01  2.5295e-02 -1.6068e-01 -1.3878e-01\n",
            "  4.8686e-01  1.1420e+00  5.6195e-02 -7.3306e-01  8.6932e-01 -3.5892e-01\n",
            " -5.1877e-01  9.0402e-01  4.9249e-01 -1.4915e-01  4.8493e-02  2.6096e-01\n",
            "  1.1352e-01  4.1275e-01  5.3803e-01 -4.4950e-01  8.5733e-02  9.1184e-02\n",
            "  5.0177e-03 -3.4645e-01 -1.1058e-01 -2.2235e-01 -6.5290e-01 -5.1838e-02\n",
            "  5.3791e-01 -8.1040e-01 -1.8253e-01  2.4194e-01  5.4855e-01  8.7731e-01\n",
            "  2.2165e-01 -2.7124e+00  4.9405e-01  4.4703e-01  5.5882e-01  2.6076e-01\n",
            "  2.3760e-01  1.0668e+00 -5.6971e-01 -6.4960e-01  3.3511e-01  3.4609e-01\n",
            "  1.1033e+00  8.5261e-02  2.4847e-02 -4.5453e-01  7.7012e-02  2.1321e-01\n",
            "  1.0444e-01  6.7157e-02 -3.4261e-01  8.5534e-01  1.3361e-01 -4.3296e-01\n",
            " -5.6726e-01 -2.1348e-01 -3.3277e-01  3.4351e-01  3.2164e-01  4.4527e-01\n",
            " -1.3208e+00 -1.3270e-01 -7.0820e-01 -4.8472e-01 -6.9396e-01 -2.6080e-01\n",
            " -4.7099e-01 -5.7492e-02  9.3587e-02  4.0006e-01 -4.3419e-01 -2.7364e-01\n",
            " -7.7017e-01 -8.4028e-01 -1.5620e-03  6.2223e-01]\n",
            "\n",
            "10 most similar words to:  love [('me', 0.7382813692092896), ('passion', 0.735213577747345), ('my', 0.7327208518981934), ('life', 0.7287957668304443), ('dream', 0.7267670035362244), ('you', 0.7181724309921265), ('always', 0.7111519575119019), ('wonder', 0.7094581127166748), ('i', 0.7084634304046631), ('dreams', 0.7067317962646484)]\n",
            "Vector of: friendship [ 0.51813    0.79066   -0.15113    0.99996    0.27108   -0.39643\n",
            " -0.02862   -0.54229   -0.39664   -0.43319   -1.2005    -0.20581\n",
            "  1.0134     0.42438    0.10026   -0.34857    0.86265   -0.60788\n",
            " -0.9637     0.059907  -0.10289    0.090389  -0.54098    0.0088074\n",
            "  0.62559    0.37331   -0.050293  -0.39242    0.55023   -0.15198\n",
            " -0.57178    1.389      0.56011   -0.37128    0.067883   0.036163\n",
            "  0.99142   -0.078035  -0.45492   -0.0037057  0.004526  -0.094208\n",
            "  0.72276   -0.41788    0.085468  -0.46014    0.053003   0.54909\n",
            "  0.50657   -0.60181   -0.3263    -0.14771    0.057282   0.94446\n",
            "  0.55029   -1.6908     0.09723   -0.040324   0.59984    0.23109\n",
            " -0.53376    0.58561   -0.29401   -0.44921   -0.50741    0.67845\n",
            "  0.19392    0.61172    0.59049   -0.72692    0.96463    0.073383\n",
            "  0.72056   -0.91361    0.39574    0.93179    0.3528    -0.40029\n",
            " -1.1393     0.56441   -0.37816    1.1779     1.2567     0.10104\n",
            " -0.47704    1.0889    -0.55661   -0.0022083 -0.37627   -0.66177\n",
            " -0.40627    0.12851   -0.33155    0.12291   -0.0092895  0.11307\n",
            " -0.43478   -0.68978   -0.55796    0.94777  ]\n",
            "\n",
            "10 most similar words to:  friendship [('relationship', 0.737876832485199), ('relations', 0.7214967608451843), ('understanding', 0.6997854709625244), ('bilateral', 0.6785919070243835), ('ties', 0.6776943802833557), ('friendships', 0.6729942560195923), ('cooperation', 0.6710678935050964), ('partnership', 0.6621981263160706), ('mutual', 0.653221607208252), ('neighborly', 0.6428439021110535)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGpixdY5kAQ"
      },
      "source": [
        "**TF-IDF Weighted Average**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekU6VXNwur5l"
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "\n",
        "    def __init__(self, word_model):\n",
        "\n",
        "        self.word_model = word_model\n",
        "        self.word_idf_weight = None\n",
        "        self.vector_size = word_model.wv.vector_size\n",
        "\n",
        "    def fit(self, docs):  # comply with scikit-learn transformer requirement\n",
        "        \"\"\"\n",
        "        Fit in a list of docs, which had been preprocessed and tokenized,\n",
        "        such as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
        "        Then build up a tfidf model to compute each word's idf as its weight.\n",
        "        Noted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
        "        :param: pre_processed_docs: list of docs, which are tokenized\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "\n",
        "        text_docs = []\n",
        "        for doc in docs:\n",
        "            text_docs.append(\" \".join(doc))\n",
        "\n",
        "        tfidf = TfidfVectorizer() # default 1-gram \n",
        "        tfidf.fit(text_docs)  # must be list of text string\n",
        "\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of\n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
        "        self.word_idf_weight = defaultdict(lambda: max_idf,\n",
        "                                           [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
        "        return self\n",
        "\n",
        "    def transform(self, docs):  # comply with scikit-learn transformer requirement\n",
        "        doc_word_vector = self.word_average_list(docs)\n",
        "        return doc_word_vector\n",
        "\n",
        "\n",
        "\n",
        "    def word_average(self, sent):\n",
        "        \"\"\"\n",
        "        Compute average word vector for a single doc/sentence.\n",
        "        :param sent: list of sentence tokens\n",
        "        :return:\n",
        "            mean: float of averaging word vectors\n",
        "        \"\"\"\n",
        "\n",
        "        mean = []\n",
        "        for word in sent:\n",
        "            if word in self.word_model.wv.vocab:\n",
        "                mean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
        "\n",
        "        if not mean:  # empty words\n",
        "            # If a text is empty, return a vector of zeros.\n",
        "            logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
        "            return np.zeros(self.vector_size)\n",
        "        else:\n",
        "            mean = np.array(mean).mean(axis=0)\n",
        "            return mean\n",
        "\n",
        "\n",
        "    def word_average_list(self, docs):\n",
        "        \"\"\"\n",
        "        Compute average word vector for multiple docs, where docs had been tokenized.\n",
        "        :param docs: list of sentence in list of separated tokens\n",
        "        :return:\n",
        "            array of average word vector in shape (len(docs),)\n",
        "        \"\"\"\n",
        "        return np.vstack([self.word_average(sent) for sent in docs])        "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYZqtRoTv8fy",
        "outputId": "756a1ae1-6b7d-46c0-82ff-18d663abe00b"
      },
      "source": [
        "tfidf_vec_tr = TfidfEmbeddingVectorizer(model)\n",
        "\n",
        "tfidf_vec_tr.fit(train['processed_text'])  # fit tfidf model first\n",
        "X_train = tfidf_vec_tr.transform(train['processed_text'])\n",
        "y_train = train['label']\n",
        "\n",
        "tfidf_vec_tr.fit(test['processed_text'])\n",
        "X_test = tfidf_vec_tr.transform(test['processed_text'])\n",
        "y_test = test['label']"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAL_kifvzDEv",
        "outputId": "124f478e-63aa-4758-81bf-2a433acfb8d5"
      },
      "source": [
        "clf_dict = {'log reg': LogisticRegression(random_state=42), \n",
        "            'naive bayes': GaussianNB(), \n",
        "            'linear svc': LinearSVC(random_state=42),\n",
        "            'sgd classifier': SGDClassifier(random_state=42),\n",
        "            'ada boost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "            'gradient boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "            'CART': DecisionTreeClassifier(random_state=42),\n",
        "            'random forest': RandomForestClassifier(n_estimators=100, random_state=42)}\n",
        "for name, clf in clf_dict.items():\n",
        "    pred = clf.fit(X_train, y_train)\n",
        "    y_pred = pred.predict(X_test)\n",
        "    print('Accuracy of {}:'.format(name), accuracy_score(y_pred, y_test))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of log reg: 0.6741258741258741\n",
            "Accuracy of naive bayes: 0.6139860139860139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of linear svc: 0.6713286713286714\n",
            "Accuracy of sgd classifier: 0.5720279720279721\n",
            "Accuracy of ada boost: 0.6573426573426573\n",
            "Accuracy of gradient boosting: 0.6671328671328671\n",
            "Accuracy of CART: 0.5986013986013986\n",
            "Accuracy of random forest: 0.6629370629370629\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}